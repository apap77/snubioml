{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "중요한 결정을 내리기에 앞서서 한 사람의 조언을 듣기보다는 여러 사람의 조언을 모두 들어보는 게 낫겠죠?\n",
    "\n",
    "머신러닝에서도 이런 아이디어를 적용시킬 수 있습니다. 이렇게 여러 알고리즘을 한데 섞어서 만들어진 새로운 알고리즘을 메타-알고리즘이라고 합니다.\n",
    "\n",
    "이번 실습에서는 가장 유명한 메타-알고리즘인 Adaboost에 대해 공부해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "이론 시간에 배웠듯이 bagging은 하나의 데이터셋에서 샘플들을 복원추출하여 여러 개의 새로운 데이터셋을 만들어낸 뒤 이를 학습에 사용하는 방법입니다.\n",
    "\n",
    "총 S개의 새로운 데이터셋이 있다고 하면, 그 결과 S개의 classifier가 만들어지게 됩니다.\n",
    "\n",
    "이 S개의 classifier의 분류 결과 중 가장 많은 vote를 얻은 class를 채택하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Boosting은 bagging과 비슷한 방법입니다. 차이점은, Bagging의 경우 S개의 classifier가 각각 독립적으로 학습되는 반면 boosting은 순차적으로 학습된다는 점입니다.\n",
    "\n",
    "즉, boosting에서는 N번째 classifier의 학습은 이미 학습이 완료된 1~N-1번째 classifier의 성능에 따라 달라지는 것입니다.\n",
    "\n",
    "이 때 1~N-1번째 classifier가 잘못 분류한 샘플에 대해 보다 집중하여 학습함으로써 성능의 향상을 꾀합니다.\n",
    "\n",
    "Boosting에는 다양한 버전이 있지만, 이번에는 가장 유명한 버전인 Adaboost에 대해서 공부해볼 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost : Improving the classifier by focusing on errors\n",
    "\n",
    "흥미로운 질문이 하나 있습니다.\n",
    "\n",
    "<b>'약한' 분류기 여러 개를 가지고 '강한' 분류기를 만들 수 있을까요?</b>\n",
    "\n",
    "여기서 약한 분류기란, 이를테면 binary classification 문제에 대해서 <i>동전을 던져 결정하는 것보다</i> 아주 약간 좋은 성능을 보이는 분류기를 말합니다.\n",
    "\n",
    "AdaBoost는 Adaptive Boosting의 준말입니다. AdaBoost는 아래와 같이 작동합니다:\n",
    "\n",
    "먼저 training 데이터셋의 모든 샘플에 모두 동일한 weight가 부여됩니다. 우리는 앞으로 이 weight vector를 $ D $라고 부를 것입니다.\n",
    "\n",
    "최초로 하나의 약한 분류기가 학습됩니다. 이 약한 분류기에 대한 error를 계산합니다.\n",
    "\n",
    "두 번째로 약한 분류기를 학습하는데, 이 때 각각의 샘플에 대한 weight가 조정됩니다.\n",
    "\n",
    "첫 번째 분류기에 의해 제대로 분류된 샘플의 weight은 작게, 그렇지 않은 샘플의 weight은 크게 만드는 것이죠.\n",
    "\n",
    "분류기 전체의 분류값을 얻기 위해서 AdaBoost 알고리즘은 각 분류기의 error에 기반한 $ \\alpha $값들을 분류기마다 할당합니다.\n",
    "\n",
    "error $ \\epsilon $는 아래와 같이 주어집니다.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\epsilon = \\frac{\\text{number of incorrectly classified examples}}{\\text{total number of examples}}\n",
    "\\end{equation*}\n",
    "\n",
    "$ \\alpha $는 아래와 같이 주어집니다.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha = \\frac{1}{2}ln(\\frac{1-\\epsilon}{\\epsilon})\n",
    "\\end{equation*}\n",
    "\n",
    "$ \\alpha $값을 계산했다면, 이제 weight vector $ D $를 update해야 합니다.\n",
    "\n",
    "\\begin{equation*}\n",
    "D^{(i+1)}_i = \\frac{D^{(i)}_ie^{-\\alpha}}{Sum(D)} \\text{if correct}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "D^{(i+1)}_i = \\frac{D^{(i)}_ie^{\\alpha}}{Sum(D)} \\text{if incorrect}\n",
    "\\end{equation*}\n",
    "\n",
    "$ D $를 update했다면 AdaBoost의 다음 iteration을 이 과정을 반복함으로써 수행하게 됩니다.\n",
    "\n",
    "이 반복은 training error가 0이 되거나 weak classifier의 개수가 미리 설정한 값에 도달할 때까지 수행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a weak learner with a decision stump\n",
    "\n",
    "<i>Decision stump</i>는 이름에서도 알 수 있듯이 아주 간단한 버전의 decision tree라고 생각하면 됩니다.\n",
    "\n",
    "우리는 weak classifier로 decision stump를 이용할 것입니다.\n",
    "\n",
    "AdaBoost에 대한 코드를 작성하기 전에, decision stump를 생성하는 함수를 작성합시다.\n",
    "\n",
    "간단한 데이터셋을 불러옵시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22063cbb748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def loadSimpData():\n",
    "    datMat = np.matrix([[1., 2.1], [1.5, 1.6], [1.3, 1.], [1., 1.], [2., 1.]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat, classLabels\n",
    "\n",
    "simpleData, simpleLabels = loadSimpData()\n",
    "\n",
    "plt.plot(simpleData[[0, 1, 4], 0], simpleData[[0, 1, 4], 1], 'ro')\n",
    "plt.plot(simpleData[[2, 3], 0], simpleData[[2, 3], 1], 'bs')\n",
    "plt.axis([0.8, 2.2, 0.8, 2.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stump가 주어지면 이를 바탕으로 데이터셋을 분류하는 함수 <b>stumpClassify()</b>와,\n",
    "\n",
    "가중치를 반영한 error값을 최소로 만드는 stump를 구하는 함수 <b>buildStump()</b>를 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stumpClassify(dataMatrix, dimension, threshVal, threshIneq):\n",
    "    retArray = np.ones((dataMatrix.shape[0], 1))\n",
    "    if threshIneq == 'lt':  # less than\n",
    "        retArray[dataMatrix[:, dimension] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:, dimension] > threshVal] = -1.0\n",
    "    return retArray\n",
    "\n",
    "def buildStump(dataArr, classLabels, D):\n",
    "    dataMatrix = np.mat(dataArr)\n",
    "    labelMat = np.mat(classLabels).T\n",
    "    \n",
    "    m, n = dataMatrix.shape\n",
    "    \n",
    "    numSteps = 10.0\n",
    "    bestStump = {}\n",
    "    bestClasEst = np.zeros((m, 1))\n",
    "    minError = np.inf\n",
    "    \n",
    "    for i in range(n):  # for every feature\n",
    "        rangeMin = dataMatrix[:, i].min()\n",
    "        rangeMax = dataMatrix[:, i].max()\n",
    "        \n",
    "        stepSize = (rangeMax - rangeMin) / numSteps\n",
    "        for j in range(-1, int(numSteps) + 1):\n",
    "            for inequal in ['lt', 'gt']:\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)\n",
    "                errArr = np.mat(np.ones((m, 1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = D.T * errArr\n",
    "                \n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "                # ENDIF\n",
    "            # ENDFOR - inequal\n",
    "        # ENDFOR - j\n",
    "    # ENDFOR - i\n",
    "    return bestStump, minError, bestClasEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'ineq': 'lt', 'thresh': 1.3}, matrix([[ 0.2]]), array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = np.mat(np.ones((5, 1)) / 5)\n",
    "buildStump(simpleData, simpleLabels, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 AdaBoost를 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr, classLabels, numIt=40):\n",
    "    weakClassArr = []\n",
    "    m = dataArr.shape[0]\n",
    "    D = np.mat(np.ones((m, 1)) / m)\n",
    "    aggClassEst = np.mat(np.zeros((m, 1)))\n",
    "    \n",
    "    # iterate for numIt times\n",
    "    for i in range(numIt):\n",
    "        # build best stump\n",
    "        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n",
    "        print(\"D:\", D.T)\n",
    "        \n",
    "        # compute alpha\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        print(\"classEst: \", classEst.T)\n",
    "        \n",
    "        # update D\n",
    "        expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)\n",
    "        D = np.multiply(D, np.exp(expon))\n",
    "        D = D / D.sum()\n",
    "        \n",
    "        aggClassEst += alpha * classEst\n",
    "        print(\"aggClassEst: \", aggClassEst.T)\n",
    "        \n",
    "        aggErrors = np.sum(np.sign(aggClassEst) != np.mat(classLabels).T)\n",
    "        errorRate = aggErrors.sum() / m\n",
    "        \n",
    "        print(\"Total error: \", errorRate)\n",
    "        # if all correct, break\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    # ENDFOR - i\n",
    "        \n",
    "    return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [[ 0.2  0.2  0.2  0.2  0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "Total error:  0.2\n",
      "D: [[ 0.5    0.125  0.125  0.125  0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "Total error:  0.2\n",
      "D: [[ 0.28571429  0.07142857  0.07142857  0.07142857  0.5       ]]\n",
      "classEst:  [[ 1.  1.  1.  1.  1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "Total error:  0.0\n"
     ]
    }
   ],
   "source": [
    "classifierArray = adaBoostTrainDS(simpleData, simpleLabels, 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
