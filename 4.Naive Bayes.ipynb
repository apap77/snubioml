{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "이번 시간에는 naive bayes classifier에 대해 실습을 해 봅시다.\n",
    "\n",
    "## 목차\n",
    "\n",
    "1. <a href=\"#Classifying-with-Bayesian-decision-theory\">Classifying with Bayesian decision theory</a>\n",
    "2. <a href=\"#Conditional-Probability\">Conditional Probability</a>\n",
    "3. <a href=\"#Document-classification-with-Naive-Bayes\">Document classification with Naive Bayes</a>\n",
    "4. <a href=\"#예제-:-스팸-메일-분류기\">예제 : 스팸 메일 분류기</a>\n",
    "5. <a href=\"#예제-:-개인-광고로부터-지역별-핵심-단어를-추출하기\">예제 : 개인 광고로부터 지역 별 핵심 단어를 추출하기</a>\n",
    "6. <a href=\"#실습-:-과학-분야별-뉴스-비교하기\">실습 : 과학 분야별 뉴스 비교하기</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with Bayesian decision theory\n",
    "\n",
    "<img src=\"./img/4_1.png\">\n",
    "\n",
    "위와 같은 2d 데이터셋을 bayesian 방법으로 분류해 봅시다.\n",
    "\n",
    "파란색 class를 class 1, 빨간색 class를 class 2라고 합시다.\n",
    "\n",
    "이 때, 파란색 점들과 빨간색 점들이 각각 어떤 확률분포로부터 나왔는지, 그 parameter를 어떻게든 알고 있다고 가정합니다.\n",
    "\n",
    "즉, 현재 우리는 class 1 과 class 2의 분포에 관한 통계치(평균, 공분산 등)를 알고 있습니다.\n",
    "\n",
    "한 점 (x, y)에 대해서 (x, y)가 class 1과 class 2에 속할 확률을 각각 p1(x, y), p2(x, y)라고 하면, 다음과 같이 분류할 수 있습니다.\n",
    "\n",
    "<b>p1(x, y) > p2(x, y)이면 (x, y)는 class 1에 속한다.</b>\n",
    "\n",
    "<b>p2(x, y) > p1(x, y)이면 (x, y)는 class 2에 속한다.</b>\n",
    "\n",
    "(x, y)가 속하는 class는 (x, y)가 속할 확률이 더 큰 class라는 겁니다. 이것이 bayesian 방법을 이용한 분류의 핵심입니다.\n",
    "\n",
    "여기서,\n",
    "\n",
    "p1(x, y)는 $ p(c_1 | x, y) $, p2(x, y)는 $ p(c_2 | x, y) $ 와 같이 쓰일 수 있습니다. \n",
    "\n",
    "이를 풀어서 써보면, \"x와 y가 주어졌을 때, (x, y)가 class 1/class 2에 속할 확률\" 이 되기 때문입니다.\n",
    "\n",
    "그렇다면 어떻게 $ p(c_1 | x, y) $, $ p(c_2 | x, y) $를 계산할 수 있을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "조건부확률에 대해서 간단히 복습해봅시다.\n",
    "\n",
    "\\begin{equation*}\n",
    "p(c|x) = \\frac{p(x|c)p(c)}{p(x)}\n",
    "\\end{equation*}\n",
    "\n",
    "입니다.\n",
    "\n",
    "확장하면\n",
    "\n",
    "\\begin{equation*}\n",
    "p(c_i|x, y) = \\frac{p(x, y|c_i)p(c_i)}{p(x, y)}\n",
    "\\end{equation*}\n",
    "\n",
    "도 성립합니다.\n",
    "\n",
    "아, 이제 $ p(c_i|x, y) $를 직접 구하지 않고도 우변의 세 확률을 계산하면 $ p(c_i|x, y) $를 구할 수 있겠군요!\n",
    "\n",
    "각각의 항의 의미를 풀어서 써보면 다음과 같습니다.\n",
    "\n",
    "$ p(x, y|c_i) $ : \"class i 내에서 임의의 점을 골랐을 때 (x, y)가 선택될 확률\" (= class i의 확률분포)\n",
    "\n",
    "$ p(c_i) $ : \"임의의 점을 골랐을 때 class i의 점이 선택될 확률\" (= 전체 class에서 class i가 차지하는 비율)\n",
    "\n",
    "$ p(x, y) $ : \"임의의 점을 골랐을 때 (x, y)가 선택될 확률\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classification with Naive Bayes\n",
    "\n",
    "자동 문서 분류는 머신 러닝에서 다루는 중요한 분야 중 하나입니다.\n",
    "\n",
    "문서 분류에서는 하나의 문서가 하나의 데이터가 되며, 문서 내의 요소들(단어, 문장 등)이 feature가 됩니다.\n",
    "\n",
    "즉, <b>어떤 단어의 존재 유무(0/1)를 벡터로 표현</b>하게 되면 그것을 해당 문서를 나타내는 feature로 생각할 수 있게 되는 것입니다.\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes Classifier는 문서 분류에서 자주 사용되는 알고리즘입니다.\n",
    "\n",
    "아까 단어의 존재 유무 벡터가 feature가 된다고 했는데, 그렇다면 그 벡터의 크기는 얼마가 되어야 할까요?\n",
    "\n",
    "영어에서 사용되는 단어의 총 개수(단어장)는 약 500,000개라고 합니다. \n",
    "\n",
    "그러니까 50만개의 원소를 갖는 벡터가 하나의 데이터를 나타내는 feature가 되는 것입니다.\n",
    "\n",
    "이 벡터들을 가지고 학습을 시키면 실제 세계의 문서들을 잘 분류할 수 있겠군요! 하지만 여기서는 간단한 단어장만을 다루겠습니다.\n",
    "\n",
    "단어장의 크기가 1000 이라고 합시다. 이를 가지고 좋은 확률분포를 만들어 내려면 충분한 데이터가 필요합니다.\n",
    "\n",
    "통계적으로, 하나의 feature에 대해 $ N $개의 데이터로 좋은 확률분포를 만들어내기에 충분하다면 10개의 feature에 대해서는 $ N^{10} $개, 1000개의 feature에 대해서는 $ N^{1000} $개의 데이터가 필요하다고 합니다.\n",
    "\n",
    "즉 만약 단어장의 크기가 1000이고, $ N = 10 $이라고 생각하면 $ 10^{1000} $개의 데이터가 필요하다는 것입니다.\n",
    "\n",
    "하지만 Naive Bayes에서는 feature들간의 독립을 가정합니다. 이렇게 하면 $ N^{1000} $개의 데이터가 필요하던 것이 $ 1000N $으로 줄어들게 된다고 합니다.\n",
    "\n",
    "feature들간의 독립을 가정한다는 것이 무슨 의미일까요? 문서 분류에서는 feature가 특정 단어의 존재 유무라고 했지요? 이들이 서로 독립이라는 것은, 이를테면 <i>bacon</i>이라는 단어가 <i>unhealthy</i>와 같이 등장할 확률과 <i>delicious</i>와 같이 등장할 확률이 같다는 의미입니다.\n",
    "\n",
    "하지만 <i>bacon</i>은 <i>delicious</i>와 같이 등장할 확률이 <i>unhealthy</i>와 같이 등장할 확률보다 높으므로(?) feature간의 독립성을 가정하는 것은 말 그대로 naive한 가정일 뿐입니다. 그래서 Naive Bayes인 것이죠.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 테스트 데이터셋을 만들어 봅시다. 공격적인 댓글과, 그렇지 않은 댓글을 분류해봅시다.\n",
    "\n",
    "악성 댓글을 필터링하는데 사용될 수 있겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    # example message postings\n",
    "    messages = ['my dog has flea problems help please', # non abusive - 0\n",
    "                       'maybe not take him to dog park stupid', # abusive - 1\n",
    "                       'my dalmation is so cute I love him',   # non abusive - 0\n",
    "                       'stop posting stupid worthless garbage',  # abusive - 1\n",
    "                       'mr licks ate my steak how to stop him', # non abusive - 0\n",
    "                       'quit buying worthless dog food stupid'] # abusive - 1\n",
    "    # split each message by space\n",
    "    tokenizedMessages = [message.split() for message in messages]\n",
    "    \n",
    "    classVec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not\n",
    "    \n",
    "    return tokenizedMessages, classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터셋으로부터 단어장을 만드는 함수 <b>createVocabList()</b>를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) # empty set\n",
    "    \n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # iteratively enlarge vocabulary set\n",
    "        \n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어의 list를 받아서, 우리의 단어장에 있는 단어이면 1, 없는 단어이면 0으로 해당 원소를 세팅한 벡터를 반환하는 함수 <b>setOfWords2Vec()</b>을 작성합니다.\n",
    "\n",
    "ex) 단어장 ['I', 'like', 'you', 'hate', 'dog', 'cat'] 일 때,\n",
    "\n",
    "input ['I', 'like', 'dog'] 이면, [1, 1, 0, 0, 1, 0] 반환\n",
    "\n",
    "input ['I', 'hate', 'you'] 이면, [1, 0, 1, 1, 0, 0] 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    # for each word in input set,\n",
    "    for word in inputSet:\n",
    "        # if the word is in my vocabulary list, make corresponding element of the vector 1\n",
    "        # else print warning message\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word '%s' is not in my Vocabulary!\" % word)\n",
    "    \n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
      "[0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 로드합니다.\n",
    "myMessages, myClasses = loadDataSet()\n",
    "\n",
    "print(myMessages)\n",
    "print(myClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dalmation', 'stop', 'quit', 'I', 'so', 'mr', 'not', 'worthless', 'take', 'flea', 'is', 'please', 'garbage', 'cute', 'love', 'steak', 'my', 'licks', 'park', 'has', 'him', 'posting', 'ate', 'food', 'stupid', 'how', 'buying', 'maybe', 'dog', 'help', 'problems', 'to']\n"
     ]
    }
   ],
   "source": [
    "# 단어장을 만들고,\n",
    "myVocabList = createVocabList(myMessages)\n",
    "\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 단어의 list가 feature vector로 잘 표현되는지 확인합니다.\n",
    "print(setOfWords2Vec(myVocabList, myMessages[0])) # my dog has flea problems help please\n",
    "print()\n",
    "print(setOfWords2Vec(myVocabList, myMessages[1])) # maybe not take him to dog park stupid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes trainer\n",
    "\n",
    "\\begin{equation*}\n",
    "p(c_i|x, y) = \\frac{p(x, y|c_i)p(c_i)}{p(x, y)}\n",
    "\\end{equation*}\n",
    "\n",
    "위의 식을 다시 봅시다. 우리가 지금 다루는 데이터는 (x, y)의 2차원 벡터가 아니라 단어장 크기만큼의 N차원 벡터입니다.\n",
    "\n",
    "이 벡터를 $ \\mathbf{w} $라고 표현합시다. 위 식을 다시 쓰면\n",
    "\n",
    "\\begin{equation*}\n",
    "p(c_i|\\mathbf{w}) = \\frac{p(\\mathbf{w}|c_i)p(c_i)}{p(\\mathbf{w})}\n",
    "\\end{equation*}\n",
    "\n",
    "이 됩니다.\n",
    "\n",
    "우리의 Naive Bayes trainer 함수 <b>trainNB0()</b>는 우변의 항들을 계산해줄 겁니다.\n",
    "\n",
    "$ {p(\\mathbf{w})} $ 는 $ p(c_0|\\mathbf{w}) $을 계산할때도, $ p(c_1|\\mathbf{w}) $을 계산할때도 똑같을 것이므로 신경쓰지 않아도 됩니다.\n",
    "\n",
    "$ p(c_i) $는 training set에서 class i의 개수를 전체 데이터의 개수로 나누어 구할 수 있습니다.\n",
    "\n",
    "문제는 $ p(\\mathbf{w}|c_i) $인데요, class i 내에서 단어벡터 $ \\mathbf{w} $가 나타날 확률은 어떻게 구할 수 있을까요?\n",
    "\n",
    "아까 feature의 원소 간의 독립을 가정했지요? 따라서 $ p(\\mathbf{w}|c_i) = p(w_0|c_i) * p(w_1|c_i) * p(w_2|c_i) * .... * p(w_n|c_i) $가 됩니다!\n",
    "\n",
    "즉 class 별로 각각의 단어가 나타날 확률을 계산해두면 되겠네요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the first version of Naive Bayes trainer\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) # total # of docs(sentences)\n",
    "    numWords = len(trainMatrix[0]) # total # of words in my vocabulary list\n",
    "    \n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs) # (= p(c1)). Note that p(c0) = 1 - p(c1) since we only have two classes\n",
    "    # Numerator (클래스 별로 각각의 단어가 나타나면 1 증가시킵니다)\n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    # Denominator (클래스 별로 단어의 총 개수)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        # if the data belongs to class 1 (abusive)\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += np.sum(trainMatrix[i]) # add total count of words\n",
    "        # if the data belongs to class 0 (non abusive)\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += np.sum(trainMatrix[i]) # add total count of words\n",
    "            \n",
    "    # p1Vect = [p(w_0|c1), p(w_1|c1), ...., p(w_n|c1)]\n",
    "    # p0Vect = [p(w_0|c0), p(w_1|c0), ...., p(w_n|c0)]\n",
    "    p1Vect = p1Num / p1Denom\n",
    "    p0Vect = p0Num / p0Denom\n",
    "    \n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word           non abusive    abusive        \n",
      "dalmation      0.04           0.00           \n",
      "stop           0.04           0.05           \n",
      "quit           0.00           0.05           \n",
      "I              0.04           0.00           \n",
      "so             0.04           0.00           \n",
      "mr             0.04           0.00           \n",
      "not            0.00           0.05           \n",
      "worthless      0.00           0.11           \n",
      "take           0.00           0.05           \n",
      "flea           0.04           0.00           \n",
      "is             0.04           0.00           \n",
      "please         0.04           0.00           \n",
      "garbage        0.00           0.05           \n",
      "cute           0.04           0.00           \n",
      "love           0.04           0.00           \n",
      "steak          0.04           0.00           \n",
      "my             0.12           0.00           \n",
      "licks          0.04           0.00           \n",
      "park           0.00           0.05           \n",
      "has            0.04           0.00           \n",
      "him            0.08           0.05           \n",
      "posting        0.00           0.05           \n",
      "ate            0.04           0.00           \n",
      "food           0.00           0.05           \n",
      "stupid         0.00           0.16           \n",
      "how            0.04           0.00           \n",
      "buying         0.00           0.05           \n",
      "maybe          0.00           0.05           \n",
      "dog            0.04           0.11           \n",
      "help           0.04           0.00           \n",
      "problems       0.04           0.00           \n",
      "to             0.04           0.05           \n"
     ]
    }
   ],
   "source": [
    "myMessages, myClasses = loadDataSet()\n",
    "myVocabList = createVocabList(myMessages)\n",
    "\n",
    "myTrainMat = [setOfWords2Vec(myVocabList, message) for message in myMessages]\n",
    "\n",
    "p0V, p1V, pAb = trainNB0(myTrainMat, myClasses)\n",
    "\n",
    "# p0V : class 0에서 각각의 단어가 나타날 확률\n",
    "# p1V : class 1에서 각각의 단어가 나타날 확률\n",
    "# pAb : class 1(Abusive)의 비율\n",
    "\n",
    "print(\"%-15s%-15s%-15s\" % (\"word\", \"non abusive\", \"abusive\"))\n",
    "for word, class0probability, class1probability in zip(myVocabList, p0V, p1V):\n",
    "    print(\"%-15s%-15.2f%-15.2f\" % (word, class0probability, class1probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 우리의 <b>trainNB0()</b>함수에는 심각한 문제가 있습니다. 만약 $ p(\\mathbf{w}|c_i) = p(w_0|c_i) * p(w_1|c_i) * p(w_2|c_i) * .... * p(w_n|c_i) $ 이 값을 계산하는데, 중간의 어느 한 값 $ p(w_k|c_i) $가 0이라면 $ p(\\mathbf{w}|c_i) $ 값이 0이 되어버립니다! \n",
    "\n",
    "이것을 막기 위해, 클래스 별로 단어의 개수를 저장하는 p0Num과 p1Num 리스트를 1로 초기화시키고, 클래스 별 총 단어 개수인 p0Denom과 p1Denom을 2로 초기화시킵시다.\n",
    "\n",
    "또한, 단어의 종류가 많아지면, 특정 단어가 나타나는 빈도가 작아지기 때문에, 이렇게 작은 확률들을 곱하다보면 floating point 계산의 한계로 underflow 오류가 발생하게 됩니다. 즉, 실제 확률 값보다 작은 값을 반환하는 것이죠. \n",
    "\n",
    "이것을 막기 위해, 확률들을 곱하지 말고 log(확률)들을 더하는 방식으로 계산하도록 합시다.\n",
    "\n",
    "새로운 Naive Bayes trainer <b>trainNB1()</b>함수를 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB1(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    \n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # Numerator\n",
    "    p0Num = np.ones(numWords) # zeros -> ones\n",
    "    p1Num = np.ones(numWords)\n",
    "    # Denominator\n",
    "    p0Denom = 2.0 # 0.0 -> 2.0\n",
    "    p1Denom = 2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += np.sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += np.sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num / p1Denom) # added np.log()\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    \n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 바탕으로 classify하는 <b>classifyNB()</b> 함수와, 테스트를 위한 <b>testingNB()</b>함수를 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    messages, classes = loadDataSet()\n",
    "    vocabList = createVocabList(messages)\n",
    "    \n",
    "    trainMat =  [setOfWords2Vec(vocabList, message) for message in messages]\n",
    "    \n",
    "    p0V, p1V, pAb = trainNB1(np.array(trainMat), np.array(classes))\n",
    "    \n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(vocabList, testEntry))\n",
    "    \n",
    "    print(testEntry, \"classified as : \", classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    \n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(vocabList, testEntry))\n",
    "    \n",
    "    print(testEntry, \"classified as : \", classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as :  0\n",
      "['stupid', 'garbage'] classified as :  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서는 <b>setOfWords2Vec()</b>함수를 이용하여 단어의 존재 유무를 feature vector로 나타냈지만, 다른 방법도 존재합니다.\n",
    "\n",
    "<b>bagOfWords2Vec()</b> 함수는 각각의 단어가 출현한 횟수를 세어서 벡터로 반환해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text\n",
    "\n",
    "실제 세계의 문서들을 다루기 전에 이 문서들을 각각의 단어로 쪼개는 tokenizing 방법을 알아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon.']\n"
     ]
    }
   ],
   "source": [
    "mySentence = \"This book is the best book on Python or M.L. I have ever laid eyes upon.\"\n",
    "\n",
    "print(mySentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어들로 잘 쪼개졌지만, 온점이 단어에 포함된 것을 확인할 수 있습니다. (upon.)\n",
    "\n",
    "regular expression을 사용하여, 알파벳이나 숫자가 아닌 것들(\"\\\\\\W\")을 기준으로 문장을 쪼개면 해결됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', '', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regEx = re.compile(\"\\\\W\")\n",
    "tokens = regEx.split(mySentence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막에 아무것도 없는 단어가 있군요! 이런 녀석들을 제거하기 위해, 길이가 0보다 큰 단어들만 취합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M', 'L', 'I', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print([token for token in tokens if len(token) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장의 일관성을 위해, 모든 단어를 소문자로 바꿔줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever', 'laid', 'eyes', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print([token.lower() for token in tokens if len(token) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "길이가 너무 짧은 단어들은 도움이 안 될 가능성이 큽니다. 길이가 2보다 큰 단어들만 취하는게 좋겠군요.\n",
    "\n",
    "<b>textParse()</b>함수를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    tokens = re.split(r'\\W', bigString)\n",
    "    return [tok.lower() for tok in tokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 : 스팸 메일 분류기\n",
    "\n",
    "이제 스팸 메일 분류기를 만들어 봅시다.\n",
    "\n",
    "<a href=\"./datasets/email.zip\">데이터셋 다운로드</a>\n",
    "\n",
    "이 데이터셋은 스팸 메일과 스팸 메일이 아닌 메일(ham) 25개씩을 포함하고 있습니다.\n",
    "\n",
    "이 중 랜덤하게 10개를 골라 test set으로 활용하여 cross-validation test를 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spamTest(verbose=False):\n",
    "    docList, classList, fullText, = [], [], []\n",
    "    # ham & spam have 25 mails each (1~25)\n",
    "    for i in range(1, 26):\n",
    "        # spam\n",
    "        wordList = textParse(open(\"./datasets/email/spam/%d.txt\" % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        # non-spam\n",
    "        wordList = textParse(open(\"./datasets/email/ham/%d.txt\" % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    # create vocab list\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    \n",
    "    # sample 10 for test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    \n",
    "    trainMat, trainClasses = [], []\n",
    "    \n",
    "    # train naive bayes classifier\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB1(np.array(trainMat), np.array(trainClasses))\n",
    "    \n",
    "    # compute error rate\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    if verbose:\n",
    "        print(\"the error rate is \", float(errorCount)/len(testSet))\n",
    "    \n",
    "    return float(errorCount)/len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is  0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamTest(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 번으로는 충분하지 않습니다. 10번 정도 반복하여 error rate의 평균을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.1, 0.1]\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "errors = [spamTest() for _ in range(10)]\n",
    "\n",
    "print(errors)\n",
    "print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 : 개인 광고로부터 지역별 핵심 단어를 추출하기\n",
    "\n",
    "이번 예제는 재밌습니다. \n",
    "\n",
    "지금까지 우리는 악성 댓글과 스팸 메일을 분류해내는 데 Naive Bayes classifier를 활용했습니다.\n",
    "\n",
    "Naive Bayes classifier는 그 활용 가능성이 무궁무진합니다.\n",
    "\n",
    "예를 들어, 어떤 사람은 자신이 좋아했던 혹은 좋아하지 않았던 여성들의 SNS 프로필을 학습시켜서 처음 보는 사람의 프로필만 가지고 그 사람을 자신이 좋아할지 안 좋아할지 예측하게 해 보기도 했다고 합니다.\n",
    "\n",
    "이번 예제에서는 미국의 두 도시 뉴욕과 샌프란시스코의 개인 광고를 비교해볼 것입니다.\n",
    "\n",
    "<a href=\"http://www.craigslist.org\">Craigslist</a>는 간단하게 말해서 온라인 벼룩시장 같은 느낌의 사이트입니다. 개인이 게시판에 짤막한 광고를 올리는 형태이죠.\n",
    "\n",
    "우리는 이 사이트에 올라온 글들을 RSS(Rich Site Summary) feed의 형태로 받아 와서 parsing할 것입니다. 이를 위해 python library인 feedparser가 필요합니다.\n",
    "\n",
    "설치는 pip install feedparser를 치고 성공하기를 기도하면 됩니다. 만약 안 된다면 구글링을 해서 소스 파일을 받아 직접 설치하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 먼저 feedparser를 import 하고,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴욕의 craigslist 광고를 받아옵니다. /stp/는 strictly platonic으로, 다른 게시판에는 좀 선정적인 광고들도 올라오기 때문에 이 게시판을 사용합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny = feedparser.parse(\"http://newyork.craigslist.org/stp/index.rss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 entry를 확인해봅시다. Dictionary 형태로 parsing된 것을 확인할 수 있습니다. 주목해야 할 부분은 'summary' 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dc_source': 'http://newyork.craigslist.org/mnh/stp/5689915740.html',\n",
       " 'dc_type': 'text',\n",
       " 'enc_enclosure': {'resource': 'http://images.craigslist.org/00i0i_dXxb8aaeioN_300x300.jpg',\n",
       "  'type': 'image/jpeg'},\n",
       " 'id': 'http://newyork.craigslist.org/mnh/stp/5689915740.html',\n",
       " 'language': 'en-us',\n",
       " 'link': 'http://newyork.craigslist.org/mnh/stp/5689915740.html',\n",
       " 'links': [{'href': 'http://newyork.craigslist.org/mnh/stp/5689915740.html',\n",
       "   'rel': 'alternate',\n",
       "   'type': 'text/html'}],\n",
       " 'published': '2016-07-19T02:00:00-04:00',\n",
       " 'published_parsed': time.struct_time(tm_year=2016, tm_mon=7, tm_mday=19, tm_hour=6, tm_min=0, tm_sec=0, tm_wday=1, tm_yday=201, tm_isdst=0),\n",
       " 'rights': 'copyright 2016 craiglist',\n",
       " 'rights_detail': {'base': 'http://newyork.craigslist.org/search/stp?format=rss',\n",
       "  'language': None,\n",
       "  'type': 'text/plain',\n",
       "  'value': 'copyright 2016 craiglist'},\n",
       " 'summary': 'Hello! \\nI am a writer and a poet (a male), and I am seeking an older woman who can act as an ally to me in my creative endeavors. I have always gotten along best with women on this basis, and I feel the lacking in my life right now since I have no fe [...]',\n",
       " 'summary_detail': {'base': 'http://newyork.craigslist.org/search/stp?format=rss',\n",
       "  'language': None,\n",
       "  'type': 'text/html',\n",
       "  'value': 'Hello! \\nI am a writer and a poet (a male), and I am seeking an older woman who can act as an ally to me in my creative endeavors. I have always gotten along best with women on this basis, and I feel the lacking in my life right now since I have no fe [...]'},\n",
       " 'title': 'SEEKING AN OLDER WOMAN OF MEANS AND MIND - m4w',\n",
       " 'title_detail': {'base': 'http://newyork.craigslist.org/search/stp?format=rss',\n",
       "  'language': None,\n",
       "  'type': 'text/plain',\n",
       "  'value': 'SEEKING AN OLDER WOMAN OF MEANS AND MIND - m4w'},\n",
       " 'updated': '2016-07-19T02:00:00-04:00',\n",
       " 'updated_parsed': time.struct_time(tm_year=2016, tm_mon=7, tm_mday=19, tm_hour=6, tm_min=0, tm_sec=0, tm_wday=1, tm_yday=201, tm_isdst=0)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny['entries'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴욕과 샌프란시스코의 광고를 비교해보기 전에, 간단한 helper function을 작성합시다. \n",
    "\n",
    "<b>calcMostFreq(vocabList, fullText)</b>는 vocabList에 있는 단어들 중 fullText에 가장 많이 나타나는 30개의 단어를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList, fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedFreq[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로는 스팸 메일 분류기와 비슷합니다. 그러나 가장 많이 나타나는 30개의 단어를 vocabList에서 삭제한 뒤에 분류를 합니다. 그 이유는 무엇일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localWords(feed1, feed0):\n",
    "    import feedparser\n",
    "    docList, classList, fullText = [], [], []\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        \n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    ### remove top 30 frequent words from vocabList\n",
    "    vocabList = createVocabList(docList)\n",
    "    top30Words = calcMostFreq(vocabList, fullText)\n",
    "    \n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList:\n",
    "            vocabList.remove(pairW[0])\n",
    "    \n",
    "    trainingSet = list(range(2 * minLen))\n",
    "    testSet = []\n",
    "    # choose 20 for test set\n",
    "    for i in range(20):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    \n",
    "    # train our classifier\n",
    "    trainMat, trainClasses = [], []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    \n",
    "    p0V, p1V, pSpam = trainNB1(np.array(trainMat), np.array(trainClasses))\n",
    "    \n",
    "    # test\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = bagOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    \n",
    "    print(\"the error rate is: \", float(errorCount) / len(testSet))\n",
    "    return vocabList, p0V, p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny = feedparser.parse(\"http://newyork.craigslist.org/stp/index.rss\")\n",
    "sf = feedparser.parse(\"http://sfbay.craigslist.org/stp/index.rss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.35\n"
     ]
    }
   ],
   "source": [
    "vocabList, pSF, pNY = localWords(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 원래 목적은 두 도시의 광고를 분류하는 것이 아니라 그냥 비교해보는 것이었습니다.\n",
    "\n",
    "어떤 단어가 그 지역의 핵심 단어인지, 다시 말해 어떤 단어가 \"도시가 주어졌을 때, 그 단어가 나타날 확률이 높은($ p(word | city) $)\"지 알아봅시다.\n",
    "\n",
    "$ p(word | city) $ 의 값들은 로그가 씌워진 형태로 p0V, p1V에 저장되어 있었죠. 이를 활용하면 됩니다.\n",
    "\n",
    "log(probability) 값이 -5.0보다 큰 단어들만 각각 골라내서 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY, topSF = [], []\n",
    "    \n",
    "    # select words if log(probability) > -5.0\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0:\n",
    "            topSF.append((vocabList[i], p0V[i]))\n",
    "        if p1V[i] > -6.0:\n",
    "            topNY.append((vocabList[i], p1V[i]))\n",
    "            \n",
    "    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True) # descending\n",
    "    print(\"===========SF============\")\n",
    "    for item in sortedSF:\n",
    "        print(item[0])\n",
    "        \n",
    "    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"===========NY============\")\n",
    "    for item in sortedNY:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.25\n",
      "===========SF============\n",
      "ladies\n",
      "please\n",
      "area\n",
      "some\n",
      "they\n",
      "more\n",
      "jose\n",
      "matter\n",
      "send\n",
      "sure\n",
      "san\n",
      "lady\n",
      "here\n",
      "fit\n",
      "cool\n",
      "been\n",
      "looks\n",
      "black\n",
      "really\n",
      "free\n",
      "says\n",
      "decent\n",
      "older\n",
      "smoke\n",
      "conversation\n",
      "friendship\n",
      "super\n",
      "night\n",
      "strictly\n",
      "race\n",
      "about\n",
      "picture\n",
      "than\n",
      "there\n",
      "need\n",
      "never\n",
      "platonic\n",
      "chill\n",
      "female\n",
      "work\n",
      "ask\n",
      "coffee\n",
      "well\n",
      "any\n",
      "when\n",
      "sweet\n",
      "able\n",
      "provide\n",
      "doing\n",
      "old\n",
      "happy\n",
      "gal\n",
      "has\n",
      "how\n",
      "working\n",
      "man\n",
      "anime\n",
      "netflix\n",
      "obey\n",
      "platonically\n",
      "single\n",
      "wished\n",
      "post\n",
      "move\n",
      "going\n",
      "fall\n",
      "play\n",
      "already\n",
      "bitch\n",
      "before\n",
      "accepted\n",
      "enjoy\n",
      "lunch\n",
      "cuddle\n",
      "stress\n",
      "sometimes\n",
      "dinner\n",
      "bay\n",
      "meeting\n",
      "preferably\n",
      "everyone\n",
      "sarcastic\n",
      "kick\n",
      "intimate\n",
      "slightly\n",
      "care\n",
      "round\n",
      "enough\n",
      "moving\n",
      "interests\n",
      "partner\n",
      "love\n",
      "feel\n",
      "bigoted\n",
      "where\n",
      "listen\n",
      "while\n",
      "invited\n",
      "own\n",
      "year\n",
      "lounges\n",
      "grad\n",
      "somewhat\n",
      "mix\n",
      "hotty\n",
      "person\n",
      "likely\n",
      "angeles\n",
      "topics\n",
      "ethnicity\n",
      "semester\n",
      "many\n",
      "smart\n",
      "drinks\n",
      "searching\n",
      "know\n",
      "cute\n",
      "lives\n",
      "loooking\n",
      "nothing\n",
      "email\n",
      "420\n",
      "smell\n",
      "girlfriend\n",
      "comes\n",
      "educated\n",
      "pics\n",
      "likes\n",
      "owl\n",
      "trying\n",
      "casual\n",
      "probably\n",
      "friend\n",
      "aren\n",
      "state\n",
      "male\n",
      "open\n",
      "talk\n",
      "first\n",
      "starts\n",
      "definitely\n",
      "school\n",
      "pet\n",
      "watch\n",
      "back\n",
      "find\n",
      "dry\n",
      "always\n",
      "speaking\n",
      "shave\n",
      "posting\n",
      "include\n",
      "nervous\n",
      "italian\n",
      "kinda\n",
      "ethnic\n",
      "hiking\n",
      "tonight\n",
      "games\n",
      "reading\n",
      "whats\n",
      "promise\n",
      "los\n",
      "title\n",
      "experience\n",
      "because\n",
      "hanging\n",
      "board\n",
      "unless\n",
      "posted\n",
      "ages\n",
      "make\n",
      "puppy\n",
      "pokemon\n",
      "between\n",
      "drink\n",
      "kind\n",
      "click\n",
      "after\n",
      "concerts\n",
      "else\n",
      "women\n",
      "stuff\n",
      "could\n",
      "shouldn\n",
      "say\n",
      "must\n",
      "relaxed\n",
      "exclude\n",
      "venue\n",
      "also\n",
      "native\n",
      "girlfriends\n",
      "daring\n",
      "their\n",
      "flag\n",
      "full\n",
      "alone\n",
      "social\n",
      "bored\n",
      "curious\n",
      "intelligent\n",
      "hangout\n",
      "want\n",
      "does\n",
      "hikes\n",
      "mixed\n",
      "job\n",
      "join\n",
      "one\n",
      "company\n",
      "humor\n",
      "process\n",
      "university\n",
      "drug\n",
      "discuss\n",
      "encounters\n",
      "but\n",
      "hey\n",
      "stressful\n",
      "environment\n",
      "network\n",
      "movies\n",
      "pals\n",
      "hea\n",
      "called\n",
      "asking\n",
      "name\n",
      "gaming\n",
      "college\n",
      "sell\n",
      "truest\n",
      "which\n",
      "nyc\n",
      "beach\n",
      "gurls\n",
      "long\n",
      "thanks\n",
      "along\n",
      "found\n",
      "livid\n",
      "boring\n",
      "most\n",
      "past\n",
      "improv\n",
      "into\n",
      "prospect\n",
      "burn\n",
      "dancing\n",
      "traveling\n",
      "discussion\n",
      "epistolary\n",
      "bowling\n",
      "type\n",
      "daddy\n",
      "around\n",
      "depends\n",
      "bronx\n",
      "massage\n",
      "heartbeat\n",
      "now\n",
      "tattooed\n",
      "will\n",
      "reply\n",
      "mind\n",
      "cry\n",
      "got\n",
      "dance\n",
      "gentlemen\n",
      "thank\n",
      "oxking08\n",
      "that\n",
      "interested\n",
      "seeing\n",
      "rushed\n",
      "guys\n",
      "loose\n",
      "pass\n",
      "properly\n",
      "strengthen\n",
      "luck\n",
      "soccer\n",
      "white\n",
      "sexy\n",
      "donate\n",
      "pal\n",
      "used\n",
      "tim\n",
      "mails\n",
      "since\n",
      "spoil\n",
      "sunday\n",
      "keepin\n",
      "face\n",
      "drama\n",
      "responding\n",
      "hyperactive\n",
      "bar\n",
      "over\n",
      "blonde\n",
      "within\n",
      "entire\n",
      "creative\n",
      "topless\n",
      "stranger\n",
      "hear\n",
      "works\n",
      "plump\n",
      "asleep\n",
      "chick\n",
      "barefoo\n",
      "interesting\n",
      "writer\n",
      "heard\n",
      "via\n",
      "gender\n",
      "online\n",
      "adventure\n",
      "seeks\n",
      "tell\n",
      "give\n",
      "fascinated\n",
      "read\n",
      "wanna\n",
      "least\n",
      "act\n",
      "word\n",
      "comforting\n",
      "practiced\n",
      "buddy\n",
      "non\n",
      "waste\n",
      "santa\n",
      "clubing\n",
      "1st\n",
      "outdoor\n",
      "active\n",
      "panties\n",
      "camping\n",
      "ally\n",
      "tonite\n",
      "deep\n",
      "hit\n",
      "them\n",
      "attitude\n",
      "lookin\n",
      "spontaneous\n",
      "she\n",
      "latino\n",
      "frequent\n",
      "knowing\n",
      "together\n",
      "once\n",
      "best\n",
      "mean\n",
      "shot\n",
      "brunch\n",
      "activities\n",
      "trip\n",
      "get\n",
      "few\n",
      "faster\n",
      "sometime\n",
      "too\n",
      "having\n",
      "underwear\n",
      "fitness\n",
      "theres\n",
      "release\n",
      "look\n",
      "city\n",
      "outdoors\n",
      "commiserate\n",
      "then\n",
      "down\n",
      "pay\n",
      "snotty\n",
      "progressed\n",
      "special\n",
      "tired\n",
      "needs\n",
      "walks\n",
      "respecting\n",
      "hour\n",
      "seeking\n",
      "bare\n",
      "services\n",
      "mature\n",
      "craigslist\n",
      "frie\n",
      "using\n",
      "tissue\n",
      "something\n",
      "maybe\n",
      "flatbush\n",
      "exercising\n",
      "appreciate\n",
      "egyptian\n",
      "righteous\n",
      "connection\n",
      "dominating\n",
      "employed\n",
      "middle\n",
      "fuck\n",
      "located\n",
      "conversant\n",
      "greenpoint\n",
      "aged\n",
      "money\n",
      "sugar\n",
      "exclusive\n",
      "week\n",
      "joys\n",
      "think\n",
      "hate\n",
      "hello\n",
      "aug\n",
      "attends\n",
      "pick\n",
      "self\n",
      "island\n",
      "huge\n",
      "coming\n",
      "hours\n",
      "was\n",
      "things\n",
      "gotten\n",
      "dominican\n",
      "fearing\n",
      "may\n",
      "anymore\n",
      "plz\n",
      "park\n",
      "mood\n",
      "hot\n",
      "music\n",
      "bed\n",
      "dude\n",
      "regular\n",
      "wants\n",
      "other\n",
      "party\n",
      "gullyharv\n",
      "wide\n",
      "restaurants\n",
      "doesn\n",
      "snack\n",
      "lonely\n",
      "sandy\n",
      "let\n",
      "hmu\n",
      "trinidadian\n",
      "cultures\n",
      "earth\n",
      "indoors\n",
      "merrit\n",
      "nude\n",
      "ass\n",
      "lil\n",
      "rosa\n",
      "curl\n",
      "degradation\n",
      "possibly\n",
      "lets\n",
      "sear\n",
      "hook\n",
      "right\n",
      "real\n",
      "variety\n",
      "shopping\n",
      "routine\n",
      "plus\n",
      "tried\n",
      "adult\n",
      "years\n",
      "day\n",
      "kik\n",
      "swimming\n",
      "sensuel\n",
      "xbox\n",
      "liberating\n",
      "body\n",
      "massages\n",
      "pen\n",
      "come\n",
      "ordinary\n",
      "follow\n",
      "someone\n",
      "little\n",
      "recieve\n",
      "couch\n",
      "pleasure\n",
      "safe\n",
      "woman\n",
      "sexual\n",
      "places\n",
      "easy\n",
      "occassionally\n",
      "charities\n",
      "running\n",
      "from\n",
      "simple\n",
      "chat\n",
      "poet\n",
      "williamsburg\n",
      "sound\n",
      "only\n",
      "help\n",
      "price\n",
      "events\n",
      "stop\n",
      "forever\n",
      "buy\n",
      "further\n",
      "pic\n",
      "fetish\n",
      "sexually\n",
      "basis\n",
      "weirdos\n",
      "occasional\n",
      "importantly\n",
      "instructions\n",
      "watching\n",
      "every\n",
      "lot\n",
      "life\n",
      "weekends\n",
      "road\n",
      "lol\n",
      "ops\n",
      "giving\n",
      "gives\n",
      "very\n",
      "near\n",
      "birthday\n",
      "kinds\n",
      "girl\n",
      "trips\n",
      "warm\n",
      "answer\n",
      "crazy\n",
      "charge\n",
      "section\n",
      "therapy\n",
      "anything\n",
      "bitches\n",
      "lake\n",
      "god\n",
      "eyes\n",
      "become\n",
      "much\n",
      "sex\n",
      "sensitive\n",
      "endeavors\n",
      "entrepreneurial\n",
      "jogs\n",
      "turned\n",
      "had\n",
      "certain\n",
      "sub\n",
      "lacking\n",
      "live\n",
      "guy\n",
      "forehead\n",
      "downs\n",
      "gurl\n",
      "shoulder\n",
      "truly\n",
      "peaceful\n",
      "inter\n",
      "worthy\n",
      "young\n",
      "gunnison\n",
      "place\n",
      "gratification\n",
      "talking\n",
      "game\n",
      "mainly\n",
      "bad\n",
      "ups\n",
      "wanted\n",
      "foot\n",
      "nor\n",
      "arms\n",
      "chested\n",
      "photo\n",
      "living\n",
      "contact\n",
      "relationship\n",
      "bit\n",
      "respectful\n",
      "each\n",
      "dress\n",
      "relationships\n",
      "experiencing\n",
      "cultured\n",
      "serve\n",
      "favorite\n",
      "mother\n",
      "same\n",
      "breast\n",
      "her\n",
      "===========NY============\n",
      "other\n",
      "guy\n",
      "now\n",
      "going\n",
      "having\n",
      "nyc\n",
      "latino\n",
      "hot\n",
      "right\n",
      "coffee\n",
      "each\n",
      "massage\n",
      "that\n",
      "interested\n",
      "free\n",
      "love\n",
      "feel\n",
      "read\n",
      "lookin\n",
      "friendship\n",
      "together\n",
      "nothing\n",
      "maybe\n",
      "hello\n",
      "let\n",
      "hmu\n",
      "kik\n",
      "platonic\n",
      "tonight\n",
      "woman\n",
      "pic\n",
      "crazy\n",
      "respectful\n",
      "name\n",
      "gaming\n",
      "old\n",
      "fit\n",
      "into\n",
      "prospect\n",
      "burn\n",
      "traveling\n",
      "discussion\n",
      "bowling\n",
      "heartbeat\n",
      "more\n",
      "cool\n",
      "reply\n",
      "mind\n",
      "fall\n",
      "dance\n",
      "gentlemen\n",
      "been\n",
      "before\n",
      "oxking08\n",
      "cuddle\n",
      "rushed\n",
      "black\n",
      "loose\n",
      "sometimes\n",
      "pass\n",
      "properly\n",
      "soccer\n",
      "meeting\n",
      "mails\n",
      "care\n",
      "keepin\n",
      "face\n",
      "drama\n",
      "responding\n",
      "decent\n",
      "over\n",
      "within\n",
      "entire\n",
      "topless\n",
      "stranger\n",
      "hear\n",
      "asleep\n",
      "gender\n",
      "online\n",
      "listen\n",
      "wanna\n",
      "least\n",
      "comforting\n",
      "practiced\n",
      "year\n",
      "waste\n",
      "clubing\n",
      "1st\n",
      "outdoor\n",
      "tonite\n",
      "deep\n",
      "brunch\n",
      "activities\n",
      "few\n",
      "faster\n",
      "sometime\n",
      "too\n",
      "send\n",
      "know\n",
      "special\n",
      "respecting\n",
      "seeking\n",
      "bare\n",
      "mature\n",
      "tissue\n",
      "strictly\n",
      "something\n",
      "flatbush\n",
      "exercising\n",
      "egyptian\n",
      "connection\n",
      "race\n",
      "employed\n",
      "middle\n",
      "fuck\n",
      "educated\n",
      "aged\n",
      "exclusive\n",
      "some\n",
      "pics\n",
      "about\n",
      "self\n",
      "huge\n",
      "things\n",
      "dominican\n",
      "fearing\n",
      "anymore\n",
      "friend\n",
      "park\n",
      "mood\n",
      "music\n",
      "bed\n",
      "dude\n",
      "wants\n",
      "party\n",
      "gullyharv\n",
      "restaurants\n",
      "open\n",
      "talk\n",
      "first\n",
      "snack\n",
      "lonely\n",
      "trinidadian\n",
      "cultures\n",
      "watch\n",
      "ass\n",
      "always\n",
      "curl\n",
      "possibly\n",
      "lets\n",
      "never\n",
      "posting\n",
      "include\n",
      "real\n",
      "shopping\n",
      "plus\n",
      "years\n",
      "swimming\n",
      "xbox\n",
      "liberating\n",
      "body\n",
      "pen\n",
      "come\n",
      "ordinary\n",
      "chill\n",
      "someone\n",
      "little\n",
      "couch\n",
      "safe\n",
      "sexual\n",
      "because\n",
      "occassionally\n",
      "make\n",
      "simple\n",
      "chat\n",
      "sound\n",
      "click\n",
      "stuff\n",
      "say\n",
      "lol\n",
      "ops\n",
      "gives\n",
      "very\n",
      "near\n",
      "girl\n",
      "warm\n",
      "lady\n",
      "answer\n",
      "therapy\n",
      "anything\n",
      "bitches\n",
      "here\n",
      "god\n",
      "become\n",
      "full\n",
      "sex\n",
      "sensitive\n",
      "entrepreneurial\n",
      "does\n",
      "live\n",
      "gurl\n",
      "truly\n",
      "peaceful\n",
      "one\n",
      "young\n",
      "company\n",
      "game\n",
      "bad\n",
      "arms\n",
      "chested\n",
      "environment\n",
      "relationship\n",
      "dress\n",
      "movies\n",
      "experiencing\n",
      "pals\n",
      "favorite\n",
      "breast\n",
      "called\n",
      "asking\n",
      "provide\n",
      "college\n",
      "doing\n",
      "sell\n",
      "happy\n",
      "truest\n",
      "which\n",
      "gal\n",
      "beach\n",
      "gurls\n",
      "has\n",
      "how\n",
      "long\n",
      "thanks\n",
      "working\n",
      "along\n",
      "found\n",
      "man\n",
      "livid\n",
      "boring\n",
      "most\n",
      "they\n",
      "past\n",
      "improv\n",
      "anime\n",
      "dancing\n",
      "netflix\n",
      "epistolary\n",
      "ladies\n",
      "type\n",
      "obey\n",
      "platonically\n",
      "daddy\n",
      "around\n",
      "depends\n",
      "single\n",
      "wished\n",
      "bronx\n",
      "tattooed\n",
      "will\n",
      "post\n",
      "move\n",
      "cry\n",
      "got\n",
      "play\n",
      "already\n",
      "bitch\n",
      "thank\n",
      "accepted\n",
      "enjoy\n",
      "looks\n",
      "lunch\n",
      "seeing\n",
      "guys\n",
      "really\n",
      "stress\n",
      "strengthen\n",
      "luck\n",
      "white\n",
      "dinner\n",
      "bay\n",
      "sexy\n",
      "please\n",
      "donate\n",
      "pal\n",
      "used\n",
      "preferably\n",
      "tim\n",
      "everyone\n",
      "sarcastic\n",
      "kick\n",
      "since\n",
      "spoil\n",
      "intimate\n",
      "slightly\n",
      "says\n",
      "round\n",
      "enough\n",
      "moving\n",
      "sunday\n",
      "hyperactive\n",
      "bar\n",
      "interests\n",
      "area\n",
      "blonde\n",
      "creative\n",
      "partner\n",
      "works\n",
      "plump\n",
      "chick\n",
      "barefoo\n",
      "interesting\n",
      "writer\n",
      "heard\n",
      "bigoted\n",
      "via\n",
      "jose\n",
      "adventure\n",
      "seeks\n",
      "tell\n",
      "give\n",
      "fascinated\n",
      "where\n",
      "act\n",
      "while\n",
      "invited\n",
      "own\n",
      "word\n",
      "buddy\n",
      "non\n",
      "santa\n",
      "lounges\n",
      "grad\n",
      "active\n",
      "older\n",
      "smoke\n",
      "panties\n",
      "camping\n",
      "ally\n",
      "somewhat\n",
      "conversation\n",
      "hit\n",
      "them\n",
      "attitude\n",
      "mix\n",
      "spontaneous\n",
      "she\n",
      "hotty\n",
      "person\n",
      "frequent\n",
      "knowing\n",
      "once\n",
      "best\n",
      "mean\n",
      "likely\n",
      "angeles\n",
      "shot\n",
      "matter\n",
      "trip\n",
      "get\n",
      "topics\n",
      "ethnicity\n",
      "semester\n",
      "underwear\n",
      "fitness\n",
      "theres\n",
      "many\n",
      "release\n",
      "smart\n",
      "look\n",
      "city\n",
      "drinks\n",
      "searching\n",
      "outdoors\n",
      "commiserate\n",
      "cute\n",
      "then\n",
      "down\n",
      "pay\n",
      "snotty\n",
      "lives\n",
      "progressed\n",
      "tired\n",
      "loooking\n",
      "needs\n",
      "walks\n",
      "hour\n",
      "services\n",
      "super\n",
      "craigslist\n",
      "night\n",
      "frie\n",
      "email\n",
      "using\n",
      "420\n",
      "smell\n",
      "girlfriend\n",
      "comes\n",
      "appreciate\n",
      "righteous\n",
      "dominating\n",
      "located\n",
      "conversant\n",
      "greenpoint\n",
      "money\n",
      "sugar\n",
      "week\n",
      "joys\n",
      "think\n",
      "hate\n",
      "aug\n",
      "likes\n",
      "sure\n",
      "attends\n",
      "pick\n",
      "island\n",
      "picture\n",
      "owl\n",
      "coming\n",
      "hours\n",
      "trying\n",
      "was\n",
      "casual\n",
      "gotten\n",
      "may\n",
      "probably\n",
      "than\n",
      "plz\n",
      "aren\n",
      "regular\n",
      "state\n",
      "male\n",
      "wide\n",
      "starts\n",
      "doesn\n",
      "definitely\n",
      "school\n",
      "sandy\n",
      "earth\n",
      "indoors\n",
      "pet\n",
      "merrit\n",
      "there\n",
      "nude\n",
      "need\n",
      "lil\n",
      "back\n",
      "find\n",
      "dry\n",
      "rosa\n",
      "speaking\n",
      "degradation\n",
      "shave\n",
      "sear\n",
      "hook\n",
      "variety\n",
      "routine\n",
      "tried\n",
      "adult\n",
      "day\n",
      "nervous\n",
      "italian\n",
      "sensuel\n",
      "kinda\n",
      "massages\n",
      "ethnic\n",
      "hiking\n",
      "games\n",
      "reading\n",
      "follow\n",
      "whats\n",
      "recieve\n",
      "pleasure\n",
      "promise\n",
      "los\n",
      "female\n",
      "san\n",
      "places\n",
      "title\n",
      "experience\n",
      "easy\n",
      "hanging\n",
      "board\n",
      "unless\n",
      "posted\n",
      "charities\n",
      "ages\n",
      "running\n",
      "from\n",
      "puppy\n",
      "pokemon\n",
      "between\n",
      "poet\n",
      "williamsburg\n",
      "drink\n",
      "only\n",
      "kind\n",
      "help\n",
      "price\n",
      "events\n",
      "stop\n",
      "forever\n",
      "buy\n",
      "further\n",
      "after\n",
      "work\n",
      "fetish\n",
      "concerts\n",
      "sexually\n",
      "basis\n",
      "else\n",
      "weirdos\n",
      "occasional\n",
      "importantly\n",
      "women\n",
      "ask\n",
      "could\n",
      "shouldn\n",
      "instructions\n",
      "watching\n",
      "every\n",
      "must\n",
      "lot\n",
      "life\n",
      "relaxed\n",
      "weekends\n",
      "road\n",
      "exclude\n",
      "venue\n",
      "also\n",
      "well\n",
      "giving\n",
      "birthday\n",
      "kinds\n",
      "trips\n",
      "native\n",
      "girlfriends\n",
      "daring\n",
      "charge\n",
      "their\n",
      "any\n",
      "section\n",
      "lake\n",
      "flag\n",
      "eyes\n",
      "much\n",
      "alone\n",
      "endeavors\n",
      "social\n",
      "bored\n",
      "jogs\n",
      "turned\n",
      "when\n",
      "curious\n",
      "intelligent\n",
      "had\n",
      "hangout\n",
      "want\n",
      "certain\n",
      "sub\n",
      "lacking\n",
      "forehead\n",
      "downs\n",
      "sweet\n",
      "hikes\n",
      "mixed\n",
      "shoulder\n",
      "job\n",
      "join\n",
      "inter\n",
      "worthy\n",
      "gunnison\n",
      "place\n",
      "humor\n",
      "gratification\n",
      "talking\n",
      "process\n",
      "university\n",
      "mainly\n",
      "ups\n",
      "wanted\n",
      "foot\n",
      "drug\n",
      "discuss\n",
      "nor\n",
      "encounters\n",
      "but\n",
      "photo\n",
      "hey\n",
      "living\n",
      "stressful\n",
      "contact\n",
      "able\n",
      "bit\n",
      "network\n",
      "relationships\n",
      "cultured\n",
      "serve\n",
      "hea\n",
      "mother\n",
      "same\n",
      "her\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 : 과학 분야별 뉴스 비교하기\n",
    "\n",
    "재미있는 실습을 준비했습니다. 과학 분야별 뉴스를 비교해서 그 분야의 핵심 단어를 추출해낼 수 있을까요?\n",
    "\n",
    "아래의 표는 어떤 두 주제에 대해서 상위 25개의 단어를 추출해본 것입니다. 각각 어떤 주제인지 아시겠나요?\n",
    "\n",
    "<img src=\"./img/4_2.png\"></img>\n",
    "\n",
    "### sciencedaily.com\n",
    "\n",
    "https://www.sciencedaily.com/newsfeeds.htm\n",
    "\n",
    "위 사이트에서 다양한 주제의 과학 분야 뉴스에 대한 rss feed를 얻을 수 있습니다.\n",
    "\n",
    "예를 들어, colon cancer에 대한 feed를 얻기 위해서는 colon cancer를 찾아 클릭한 뒤 그 주소를 <b>feedparser.parse()</b>의 인자로 넘겨주면 됩니다.\n",
    "\n",
    "colonCancer = feedparser.parse(\"https://rss.sciencedaily.com/health_medicine/colon_cancer.xml\")\n",
    "\n",
    "이렇게요.\n",
    "\n",
    "### Stop words\n",
    "\n",
    "위의 샌프란시스코와 뉴욕의 결과를 보면, can, than, very... 등의 자주 사용되는 단어가 꽤나 많이 출력되는 것을 확인할 수 있습니다.\n",
    "\n",
    "이 단어들은 우리의 분석에 아무런 도움이 안 되죠. 이런 단어들을 stop word라고 합니다.\n",
    "\n",
    "<a href=\"./datasets/stopwords.txt\">Stop word들을 모아 놓은 데이터</a>가 있습니다.\n",
    "\n",
    "### 실습\n",
    "\n",
    "제공된 stopwords.txt를 이용하여 stop word를 고려하지 않도록 우리의 문서 분류기를 수정합시다.\n",
    "\n",
    "또한, 이 문서 분류기를 이용하여 sciencedaily.com에서 자신이 흥미 있는 두 주제를 선택하여 각각의 핵심 단어를 추출해보세요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
