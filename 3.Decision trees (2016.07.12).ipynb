{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision trees\n",
    "이번 시간에는 Decision tree에 대해 실습해 봅시다.\n",
    "\n",
    "스무고개를 아시나요? 한 사람이 어떤 대상을 떠올리면, 다른 사람이 20개의 오직 예/아니오로만 대답할 수 있는 질문으로 그 대상을 알아맞히는 게임이죠.\n",
    "\n",
    "Decision tree 방법 또한 스무고개와 비슷합니다. \n",
    "\n",
    "<b>컴퓨터에게 수많은 데이터와 라벨을 보여주고, 거기서 그 라벨들을 가장 잘 분류할 수 있는 기준(질문)을 찾게 하는 것입니다.</b>\n",
    "\n",
    "이렇게 만들어진 decision tree는 새로운 데이터의 classification에 사용될 수 있습니다.\n",
    "\n",
    "예를 들어 볼까요? 가상의 스팸 메일 분류기를 만들었다고 생각해 봅시다.\n",
    "\n",
    "입력으로 메일의 내용과 그 분류 라벨(\"Email to read when bored\", \"Email from friends; read immediately\", \"Spam; dont' read\")을 주면, 다음과 같은 decision tree를 출력합니다.\n",
    "\n",
    "<img src=\"./img/3_1.jpg\"></img>\n",
    "\n",
    "이 결과로 우리는 어떤 분류 기준에 의해 각각의 class들이 분류되었는지 한눈에 알 수 있습니다. 이것이 decision tree의 장점입니다.\n",
    "\n",
    "(반면 저번 시간에 다루었던 kNN의 경우는 어떤 분류 기준에 의해 데이터가 분류되었는지 전혀 알 수 없습니다.)\n",
    "\n",
    "<b> 이번 실습의 목표는 위와 같은 tree를 만들어내는 알고리즘을 구현해 보는 것입니다. </b>\n",
    "\n",
    "데이터셋이 주어지면 적절한 feature를 찾아 데이터셋을 쪼개는 createTree() 함수를 만들어봅시다.\n",
    "\n",
    "Pseudocode는 다음과 같습니다.\n",
    "\n",
    "### createTree()\n",
    "\n",
    "데이터셋의 모든 item이 같은 class에 속하는지 확인한다.\n",
    "\n",
    "    만약 그렇다면, class label을 return한다.\n",
    "    \n",
    "    그렇지 않다면, \n",
    "        \n",
    "        데이터셋을 쪼개기에 가장 좋은 feature를 찾는다.\n",
    "        \n",
    "        데이터셋을 쪼갠다.\n",
    "        \n",
    "        branch node를 만든다.\n",
    "        \n",
    "        각각의 branch node에 대해서,\n",
    "            \n",
    "            createTree() 함수를 call 하고 그 결과를 branch node에 적용시킨다.\n",
    "\n",
    "        branch node를 return한다.\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Information gain\n",
    "\n",
    "데이터셋을 쪼개기에 가장 좋은 feature는 어떻게 찾을 수 있을까요? 정보 이론에 대한 약간의 사전 지식이 필요합니다.\n",
    "\n",
    "Entropy와 Information gain, 두 가지의 개념을 알아 두어야 합니다.\n",
    "\n",
    "### 1. 정보 이론에서의 entropy는 데이터의 <b>무질서도</b>를 의미합니다.\n",
    "\n",
    "예를 들어,\n",
    "\n",
    "10개의 데이터 모두 class A에 속하는 데이터셋과 class A 5개, class B 5개를 가진 데이터셋이 있다고 하면, 어느 쪽의 entropy가 더 높을까요?\n",
    "\n",
    "이는 수식으로 아래와 같이 정의됩니다. $ H $는 entropy, $ x_i $는 A, B, C, ...와 같은 class를 의미한다고 합시다.\n",
    "\n",
    "\\begin{equation*}\n",
    "H = -\\sum\\limits_{i=1}^n p(x_i)log_2p(x_i)\n",
    "\\end{equation*}\n",
    "\n",
    "위 수식에 따르면, 10개 모두 A인 데이터셋의 경우 H = 0이 되지만, A 5개, B 5개를 가진 데이터셋은 H = 1이 되는 것입니다.\n",
    "\n",
    "(※ 이를 제안한 Claude Shannon의 이름을 따 Shannon Entropy라고 불리기도 합니다.)\n",
    "\n",
    "### 2. Information gain(정보 이득)이란, 어떤 feature를 기준으로 데이터셋을 구분하였을 때 데이터셋이 얼마나 '잘' 구분지어지는지에 대한 값입니다.\n",
    "\n",
    "즉, 어떤 feature를 기준으로 데이터셋을 구분하였을때, 전과 후의 entropy 차이로 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon entropy를 계산하는 함수 <b>calcShannonEntropy()</b>를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "\n",
    "def calcShannonEntropy(dataSet):\n",
    "    numEntries = len(dataSet) # data의 총 개수\n",
    "    \n",
    "    # 각 label의 개수를 셉니다.\n",
    "    labelCounts = defaultdict(int)\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        labelCounts[currentLabel] += 1\n",
    "    \n",
    "    # Entropy를 계산합니다.\n",
    "    shannonEntropy = 0.0\n",
    "    for labelCount in labelCounts.values():\n",
    "        prob = float(labelCount) / numEntries\n",
    "        shannonEntropy -= prob * log(prob, 2)\n",
    "    \n",
    "    return shannonEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 테스트해보기 위해 간단한 데이터셋을 만드는 함수 <b>createDataSet()</b>을 작성하고, 테스트해봅시다.\n",
    "\n",
    "이 간단한 데이터셋은 어떤 특징을 가지는 동물이 물고기인지 아닌지 판단하는 데이터셋입니다. 이렇게 생겼습니다.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>#</td>\n",
    "<td>Can survive without coming to surface?</td>\n",
    "<td>Has flippers?</td>\n",
    "<td>Fish?</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>Yes</td>\n",
    "<td>Yes</td>\n",
    "<td>Yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>Yes</td>\n",
    "<td>Yes</td>\n",
    "<td>Yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>Yes</td>\n",
    "<td>No</td>\n",
    "<td>No</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>No</td>\n",
    "<td>Yes</td>\n",
    "<td>No</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>No</td>\n",
    "<td>Yes</td>\n",
    "<td>No</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
    "    features = ['no surfacing', 'flippers']\n",
    "    \n",
    "    return dataSet, features\n",
    "\n",
    "# create dataset\n",
    "myData, myFeatures = createDataSet()\n",
    "\n",
    "# and calculate shannon entropy\n",
    "print(calcShannonEntropy(myData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 데이터의 label을 yes도 no도 아닌 maybe로 바꿔 봅시다. 엔트로피는 어떻게 변할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new dataset\n",
    "myData1, myFeatures1 = createDataSet()\n",
    "\n",
    "# now the first one has new label 'maybe'\n",
    "myData1[0][-1] = 'maybe'\n",
    "\n",
    "# calculate shannon entropy\n",
    "print(calcShannonEntropy(myData1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 예상대로군요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 쪼개기\n",
    "\n",
    "이제 데이터셋을 쪼개는 함수 <b>splitDataSet()</b>를 만들어 봅시다.\n",
    "\n",
    "splitDataSet은 인자로 우리가 쪼갤 데이터셋(dataSet), 쪼개는 기준이 되는 feature(axis), 그리고 return되는 feature의 value(vaule), 총 3개를 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet, axis, value):\n",
    "    # dataset to be returned\n",
    "    retDataSet = []\n",
    "    \n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            # 이제 해당 feature에 대해서는 관심이 없으므로, feature를 제외한 나머지 값들만 이어붙여서\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            # 저장합니다.\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "        # END OF IF\n",
    "    # END OF FOR - featVec\n",
    "    \n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new dataset\n",
    "myData2, myFeatures2 = createDataSet()\n",
    "\n",
    "print(\"My data was\")\n",
    "print(myData2, \"\\n\")\n",
    "\n",
    "print(\"My features were\")\n",
    "print(myFeatures2, \"\\n\")\n",
    "\n",
    "print(\"Splitting on 0th feature, taking the value of 1 makes\")\n",
    "print(splitDataSet(myData2, 0, 1), \"\\n\")\n",
    "\n",
    "print(\"Splitting on 0th feature, taking the value of 0 makes\")\n",
    "print(splitDataSet(myData2, 0, 0), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가장 쪼개기 좋은 feature 찾기\n",
    "\n",
    "이제 shannon entropy도 구할 수 있고 데이터셋을 쪼갤 수도 있으니, 모든 feature에 대해서 데이터를 쪼개보면서, 가장 쪼개기 좋은 feature를 찾을 수 있습니다.\n",
    "\n",
    "가장 쪼개기 좋은 feature를 찾는 함수 <b>chooseBestFeatureToSplit()</b>을 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1 # # of features\n",
    "\n",
    "    baseEntropy = calcShannonEntropy(dataSet) # Entropy before splitting\n",
    "    \n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    \n",
    "    # iterate through features\n",
    "    for i in range(numFeatures): \n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0.0\n",
    "        \n",
    "        # iterate through unique value of the feature\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEntropy(subDataSet)\n",
    "        # END OF FOR - value\n",
    "\n",
    "        infoGain = baseEntropy - newEntropy # compute info gain and update best info gain\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    # END OF FOR - i\n",
    "    \n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 해봅시다. 우리의 데이터셋에서 가장 쪼개기 좋은 feature는 무엇일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new dataset\n",
    "myData3, myFeatures3 = createDataSet()\n",
    "\n",
    "print(\"My data was\")\n",
    "print(myData3, \"\\n\")\n",
    "\n",
    "print(\"My features were\")\n",
    "print(myFeatures3, \"\\n\")\n",
    "\n",
    "# tell me the best feature to split my dataset\n",
    "print(\"feature\", chooseBestFeatureToSplit(myData3), \"is the best feature to split on!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정말 그럴까요? 직접 데이터를 쪼개 봅시다.\n",
    "\n",
    "feature 0로 데이터를 쪼개면, 한 쪽(1이 있는)에는 2개의 'yes'와 1개의 'no', 다른 한 쪽(0이 있는)에는 2개의 'no'가 있게 됩니다.\n",
    "\n",
    "feature 1로 데이터를 쪼개면, 한 쪽(1이 있는)에는 2개의 'yes'와 2개의 'no', 다른 한 쪽(0이 있는)에는 1개의 'no'가 있게 됩니다.\n",
    "\n",
    "언뜻 보아도 feature 0로 쪼개는 것이 entropy를 더 낮출수 있는 방향입니다.\n",
    "\n",
    "감이 잘 안 오신다면, 각자 실제로 entropy를 계산해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree 만들기\n",
    "\n",
    "이제 decision tree를 만들기 위한 모든 준비가 끝났습니다! \n",
    "\n",
    "다만, 한 가지 경우를 생각해보죠. 함수 <b>splitDataSet()</b>에서 우리는 데이터셋을 쪼갤 때마다 사용된 feature를 제거했습니다.\n",
    "\n",
    "그래서 아직 데이터셋이 완벽하게 쪼개지지 않았음에도 더이상 남은 feature가 없는 경우가 발생할 수 있습니다.\n",
    "\n",
    "그 결과, tree의 leaf node에 있는 데이터의 class label들이 모두 동일하지 않게 됩니다.\n",
    "\n",
    "이 경우에 대처하기 위해서 leaf의 class label들 중 가장 많은 label을 해당 leaf의 label로 취하는 방법을 택합시다.\n",
    "\n",
    "class label들의 list가 주어지면, 그중 가장 많은 label을 취하는 <b>majorityCount()</b>함수를 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def majorityCount(classList):\n",
    "    classCounter = Counter(classList)\n",
    "    return classCounter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Decision Tree를 만드는 함수 <b>createTree</b>를 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTree(dataSet, features):\n",
    "    features = features[:] # clone features to protect original feature list\n",
    "    \n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    # if all labels are the same, return it\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # if no feature available, return major label\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCount(classList)\n",
    "    # take besf feature and it's name\n",
    "    bestFeature = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatureName = features[bestFeature]\n",
    "    # create tree\n",
    "    myTree = {bestFeatureName:{}}\n",
    "    del(features[bestFeature]) # the feature is not needed anymore\n",
    "    \n",
    "    featureValues = [example[bestFeature] for example in dataSet]\n",
    "    uniqueVals = set(featureValues)\n",
    "    # for each child node, recursively create tree\n",
    "    for value in uniqueVals:\n",
    "        subFeatures = features[:]\n",
    "        myTree[bestFeatureName][value] = createTree(splitDataSet(dataSet, bestFeature, value), subFeatures)\n",
    "    \n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myData4, myFeatures4 = createDataSet()\n",
    "\n",
    "print(\"My data was\")\n",
    "print(myData4, \"\\n\")\n",
    "\n",
    "print(\"My features were\")\n",
    "print(myFeatures4, \"\\n\")\n",
    "\n",
    "# construct decision tree with my data\n",
    "myTree4 = createTree(myData4, myFeatures4)\n",
    "\n",
    "# show me\n",
    "print(myTree4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree 시각화하기\n",
    "\n",
    "matplotlib을 이용하면 우리가 만든 decision tree를 시각화할 수 있습니다.\n",
    "\n",
    "구체적인 방법은 여기서 다루지 않겠습니다.\n",
    "\n",
    "먼저 <a href=\"./modules/treePlotter.py\">treePlotter 모듈</a>을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# the modules dir contains treePlotter.py\n",
    "sys.path.append('./modules/')\n",
    "\n",
    "import treePlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree를 그려 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myData5, myFeatures5 = createDataSet()\n",
    "\n",
    "print(\"My data was\")\n",
    "print(myData5, \"\\n\")\n",
    "\n",
    "print(\"My features were\")\n",
    "print(myFeatures5, \"\\n\")\n",
    "\n",
    "# construct decision tree with my data\n",
    "myTree5 = createTree(myData5, myFeatures5)\n",
    "\n",
    "# show me\n",
    "print(\"My tree was\")\n",
    "print(myTree5, \"\\n\")\n",
    "\n",
    "# plot it\n",
    "treePlotter.createPlot(myTree5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree에 새로운 노드를 추가해봅시다. 어떻게 그려질까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myData6, myFeatures6 = createDataSet()\n",
    "\n",
    "print(\"My data was\")\n",
    "print(myData6, \"\\n\")\n",
    "\n",
    "print(\"My features were\")\n",
    "print(myFeatures6, \"\\n\")\n",
    "\n",
    "# construct decision tree with my data\n",
    "myTree6 = createTree(myData6, myFeatures6)\n",
    "\n",
    "#### create new node ####\n",
    "myTree6['no surfacing'][3] = 'maybe'\n",
    "\n",
    "# show me\n",
    "print(\"My tree was\")\n",
    "print(myTree6, \"\\n\")\n",
    "\n",
    "# plot it\n",
    "treePlotter.createPlot(myTree6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree로 분류하기\n",
    "\n",
    "이제 데이터셋이 주어지면 decision tree를 그릴 수 있게 되었는데요, 이를 이용하여 새로운 데이터를 분류해봐야겠죠?\n",
    "\n",
    "만든 decision tree로 분류를 하는 <b>classify()</b> 함수를 작성합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(inputTree, featureNames, testVec):\n",
    "    firstFeature = list(inputTree.keys())[0]\n",
    "    subTreeDict = inputTree[firstFeature]\n",
    "    \n",
    "    featureIndex = featureNames.index(firstFeature)\n",
    "    \n",
    "    for key in subTreeDict.keys():\n",
    "        if testVec[featureIndex] == key:\n",
    "            if type(subTreeDict[key]).__name__ == \"dict\": # non-leaf node\n",
    "                classLabel = classify(subTreeDict[key], featureNames, testVec)\n",
    "            else: # leaf node\n",
    "                classLabel = subTreeDict[key]\n",
    "        \n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myData7, myFeatures7 = createDataSet()\n",
    "\n",
    "print(\"My data was\")\n",
    "print(myData7, \"\\n\")\n",
    "\n",
    "print(\"My features were\")\n",
    "print(myFeatures7, \"\\n\")\n",
    "\n",
    "# construct decision tree with my data\n",
    "myTree7 = createTree(myData7, myFeatures7)\n",
    "\n",
    "# plot it\n",
    "treePlotter.createPlot(myTree7)\n",
    "\n",
    "# test vectors\n",
    "testVec0 = [1, 0] # no surfacing : Yes, filppers : no\n",
    "testVec1 = [1, 1] # no surfacing : Yes, flippers : Yes\n",
    "\n",
    "# classify\n",
    "print(testVec0, \"classified as\", classify(myTree7, myFeatures7, testVec0))\n",
    "print(testVec1, \"classified as\", classify(myTree7, myFeatures7, testVec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 콘택트 렌즈 처방하기\n",
    "\n",
    "<a href=\"./datasets/lenses.txt\">Lenses 데이터셋</a>은 환자의 눈 상태에 따라 의사들이 어떤 콘택트 렌즈를 처방했는지에 대한 데이터셋입니다.\n",
    "\n",
    "4개의 feature column, 1개의 label column으로 총 5개의 column으로 구성됩니다.\n",
    "\n",
    "각각의 column은\n",
    "\n",
    "1. age of the patient: (1) young, (2) pre-presbyopic(젊은 노안), (3) presbyopic(노안)\n",
    "2. spectacle prescription: (1) myope(근시), (2) hypermetrope(원시)\n",
    "3. astigmatic(난시): (1) no, (2) yes \n",
    "4. tear production rate: (1) reduced, (2) normal\n",
    "\n",
    "을 나타내며, label로는 (1) no lenses, (2) soft, (3) hard 가 주어집니다.\n",
    "\n",
    "차근차근 진행해봅시다. 먼저 데이터셋을 불러와야겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./datasets/lenses.txt\", \"r\") as inFile:\n",
    "    lensesData = [line.strip().split(\"\\t\") for line in inFile.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 feature name은 데이터셋에 함께 제공되어있지 않으므로, 손으로 입력해줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lensesFeatures = ['age', 'prescript', 'astigmatic', 'tearRate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lensesTree = createTree(lensesData, lensesFeatures)\n",
    "\n",
    "print(\"Lenses tree looks like\")\n",
    "print(lensesTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뭔가 tree가 만들어진 것 같군요. 한눈에 알아보기가 어려우니 시각화하여 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "treePlotter.createPlot(lensesTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습: 독버섯??\n",
    "\n",
    "<a href=\"./datasets/mushroom.txt\">Mushroom</a> 데이터셋은 버섯의 여러 가지 특징들을 관찰하여 먹을 수 있는 버섯인지, 아닌지 구별하기 위한 데이터셋입니다.\n",
    "\n",
    "총 22개의 feature와 1개의 label(edible = e, poisonous = p)로 구성되어 있습니다.\n",
    "\n",
    "Feature들은 아래와 같이 축약되어 있습니다.\n",
    "\n",
    "1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s \n",
    "2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s \n",
    "3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y \n",
    "4. bruises?: bruises=t,no=f \n",
    "5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s \n",
    "6. gill-attachment: attached=a,descending=d,free=f,notched=n \n",
    "7. gill-spacing: close=c,crowded=w,distant=d \n",
    "8. gill-size: broad=b,narrow=n \n",
    "9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y \n",
    "10. stalk-shape: enlarging=e,tapering=t \n",
    "11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=? \n",
    "12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s \n",
    "13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s \n",
    "14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n",
    "15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y \n",
    "16. veil-type: partial=p,universal=u \n",
    "17. veil-color: brown=n,orange=o,white=w,yellow=y \n",
    "18. ring-number: none=n,one=o,two=t \n",
    "19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z \n",
    "20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y \n",
    "21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y \n",
    "22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d\n",
    "\n",
    "※ 여러분들의 편의를 위해 데이터셋의 맨 첫줄은 tab으로 구분된 feature name들로 제공됩니다.\n",
    "\n",
    "이 데이터셋을 가지고, 먹을수 있는 버섯과 독버섯을 어떤 feature들로 구분지을 수 있는지 알아보세요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
